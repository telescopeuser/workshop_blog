{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Blog: å¦‚ä½•å¼€å‘ä½¿ç”¨å¾®ä¿¡èŠå¤©æœºå™¨äººæ•™ç¨‹\n",
    "## Part 1: é—®ç­”æœºåˆ¶(è‡ªåŠ¨æ¥æ”¶ã€å›å¤æ¶ˆæ¯ã€‚æ”¯æŒç§ä¿¡ï¼Œç¾¤ç»„ï¼Œå…¬ä¼—å·ç­‰)\n",
    "* æ–‡å­—çš„æ¥æ”¶å’Œå‘é€/å›å¤\n",
    "* å›¾ç‰‡çš„æ¥æ”¶å’Œå‘é€/å›å¤\n",
    "* æ–‡ä»¶çš„æ¥æ”¶å’Œå‘é€/å›å¤\n",
    "* è¯­éŸ³çš„æ¥æ”¶å’Œå‘é€/å›å¤\n",
    "\n",
    "\n",
    "## Part 2: å›¾åƒè¯†åˆ«\n",
    "* è¯†åˆ«å›¾ç‰‡æ¶ˆæ¯ä¸­çš„ç‰©ä½“åå­—\n",
    "* è¯†åˆ«äººè„¸\n",
    "\n",
    "## Part 3: è‡ªç„¶è¯­è¨€å¤„ç†\n",
    "* è¯­éŸ³æ¶ˆæ¯çš„è¯†åˆ«ï¼Œè½¬æ¢æˆæ–‡å­—\n",
    "* æ¶ˆæ¯æ–‡å­—è½¬æˆè¯­éŸ³\n",
    "* æ¶ˆæ¯çš„å¤šè¯­è¨€äº’è¯‘\n",
    "\n",
    "\n",
    "<img src='http://www.kudosdata.com/wp-content/uploads/2016/11/cropped-KudosLogo1.png' width=30% style=\"float: right;\">\n",
    "### http://www.KudosData.com\n",
    "By: Sam.Gu@KudosData.com\n",
    "\n",
    "April, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports å¯¼å…¥ä¸€äº›åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "# Python3\n",
    "# from __future__ import division\n",
    "# Python2 unicode & float-division support:\n",
    "from __future__ import unicode_literals, division\n",
    "import time\n",
    "from wxpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting uuid of QR code.\n",
      "Downloading QR code.\n",
      "Please scan the QR code to log in.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.KudosData.com Please open QR image at: QR.png, if not shown...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please press confirm on your phone.\n",
      "Loading the contact, this may take a little while.\n",
      "Login successfully as ç™½é»‘\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/threading.py\", line 804, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python2.7/threading.py\", line 757, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/user/env/lib/python2.7/site-packages/wxpy-0.3.3-py2.7.egg/wxpy/api/bot.py\", line 399, in _listen\n",
      "    logger.info('{} stopped.'.format(self))\n",
      "  File \"/home/user/env/lib/python2.7/site-packages/wxpy-0.3.3-py2.7.egg/wxpy/api/bot.py\", line 102, in __repr__\n",
      "    return '<{}: {}>'.format(self.__class__.__name__, self.self.name)\n",
      "UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–æœºå™¨äººï¼Œæ‰«ç ç™»é™†\n",
    "bot = Bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update code base to 'utf-8'\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Friend: é…·è±†é™ªèŠå¦¹ğŸ˜˜>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æœç´¢å¥½å‹åç§°\n",
    "# my_friend = bot.friends().search('ç™½é»‘')[0]\n",
    "my_friend = bot.friends().search('MicroscopeUser')[0]\n",
    "my_friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ItchatReturnValue: {u'MsgID': u'7830462851140544367', u'LocalID': u'14921541460673', u'BaseResponse': {u'ErrMsg': u'\\u8bf7\\u6c42\\u6210\\u529f', u'Ret': 0, 'RawMsg': u'\\u8bf7\\u6c42\\u6210\\u529f'}}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_friend.send('<ç¨‹åºè‡ªåŠ¨å‘é€>\\n<ç¬¬%dæ¡ï¼Œå…±%dæ¡>\\n%s' % (1, len(df_article['sentence']), df_article['sentence'][100]))\n",
    "my_friend.send('<ç¨‹åºè‡ªåŠ¨å‘é€>\\næ­å–œæ­å–œï¼\\nçº¢åŒ…æ‹¿æ¥~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# å‘é€æ–‡æœ¬ç»™å¥½å‹\n",
    "count = len(df_article['sentence'])\n",
    "# count = 5\n",
    "for i in range(729, count):\n",
    "    try:\n",
    "        my_friend.send('<ç¬¬%dæ¡ï¼Œå…±%dæ¡>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "        print ('success rec_id = %d' % (i+1))\n",
    "        time.sleep(np.random.randint(low = 5, high = 20))\n",
    "    except:\n",
    "        tmp_rand_int = np.random.randint(low = 70, high = 130)\n",
    "        print ('retry   rec_id = %d, after %d seconds... ' % (i+1, tmp_rand_int))\n",
    "        time.sleep(tmp_rand_int)\n",
    "        try:\n",
    "            my_friend.send('<ç¬¬%dæ¡ï¼Œå…±%dæ¡>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "            print ('success rec_id = %d' % (i+1))\n",
    "            time.sleep(np.random.randint(low = 5, high = 20))\n",
    "        except:\n",
    "            tmp_rand_int = np.random.randint(low = 70, high = 130)\n",
    "            print ('retry   rec_id = %d, after %d seconds... ' % (i+1, tmp_rand_int))\n",
    "            time.sleep(tmp_rand_int)\n",
    "            my_friend.send('<ç¬¬%dæ¡ï¼Œå…±%dæ¡>\\n%s' % (i+1, count, df_article['sentence'][i]))\n",
    "            print ('success rec_id = %d' % (i+1))\n",
    "            time.sleep(np.random.randint(low = 5, high = 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img=\n",
    "imgplot = plt.imshow(mpimg.imread('QR.png'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img=\n",
    "plt.imshow(mpimg.imread('QR.png'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib\n",
    "# Force matplotlib to not use any Xwindows backend.\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img=\n",
    "imgplot = plt.imshow(mpimg.imread('QR.png'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# img=\n",
    "plt.imshow(mpimg.imread('QR.png'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import io\n",
    "# ä¸­æ–‡å­—ç¬¦å’Œè¯­è¨€å¤„ç†åº“\n",
    "import jieba\n",
    "# æœºå™¨å­¦ä¹ åº“ sklearn åˆ†ç±»å­¦ä¹ æ¨¡å‹åº“\n",
    "from sklearn.feature_extraction import DictVectorizer # æ•°æ®ç»“æ„å˜æ¢ï¼šæŠŠ Dict è½¬æ¢ä¸º ç¨€ç–çŸ©é˜µ\n",
    "# ä¸­æ–‡æ˜¾ç¤ºè®¾ç½®\n",
    "# from pylab import *  \n",
    "# mpl.rcParams['font.sans-serif'] = ['SimHei'] # æŒ‡å®šé»˜è®¤å­—ä½“  \n",
    "# mpl.rcParams['axes.unicode_minus'] = False # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜ \n",
    "# mpl.rcParams['font.size'] = 14 # è®¾ç½®å­—ä½“å¤§å°\n",
    "np.random.seed(88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from wxpy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# ä¸­æ–‡åˆ†è¯åŠŸèƒ½å°å‡½æ•°ï¼Œ è¾“å‡º å­—ç¬¦ä¸²ï¼Œ å„è¯ç»„ç”±ç©ºæ ¼åˆ†éš”\n",
    "def KudosData_word_tokenizer(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "\n",
    "    return seg_str\n",
    "# Python2\n",
    "# ä¸­æ–‡åˆ†è¯åŠŸèƒ½å°å‡½æ•°ï¼Œ è¾“å‡º å­—ç¬¦ä¸²ï¼Œ å„è¯ç»„ç”±ç©ºæ ¼åˆ†éš”\n",
    "# def KudosData_word_tokenizer(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = ' '.join(seg_token)\n",
    "#     return seg_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python3\n",
    "# ä¸­æ–‡åˆ†è¯åŠŸèƒ½å°å‡½æ•°ï¼Œ è¾“å‡º å­—ç¬¦ä¸²ï¼Œ å„è¯ç»„ç”±ç©ºæ ¼åˆ†éš”\n",
    "def KudosData_word_count(foo):\n",
    "    # remove lead & tail spaces firstly:\n",
    "    foo = foo.strip()\n",
    "    seg_token = jieba.cut(str(foo), cut_all=True)\n",
    "    seg_str = str(' '.join(seg_token)).strip()\n",
    "    seg_count = pd.value_counts(str(seg_str).lower().split(' '))\n",
    "    seg_count = seg_count.to_dict() \n",
    "    seg_count.pop('', None) # remove EMPTY dict key: ''\n",
    "#     è¾“å‡º dictionaryï¼š { key è¯ç»„ï¼Œ value è®¡æ•° }\n",
    "    #     return seg_count.to_dict()\n",
    "    return seg_count\n",
    "\n",
    "# Python2\n",
    "# ä¸­æ–‡åˆ†è¯åŠŸèƒ½å°å‡½æ•°ï¼Œ è¾“å‡º dictionaryï¼š { key è¯ç»„ï¼Œ value è®¡æ•° }\n",
    "# def KudosData_word_count(foo):\n",
    "#     seg_token = jieba.cut(foo, cut_all=True)\n",
    "#     seg_str = '^'.join(seg_token)\n",
    "#     seg_count = pd.value_counts(seg_str.lower().split('^'))\n",
    "#     return seg_count.to_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process Unicode text input\n",
    "with io.open('input_text.txt','r',encoding='utf8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "title = '''\n",
    "<Dummy Title>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sentence(text):\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linebreak_conversion_win_linux(text):\n",
    "    text = re.sub(r'\\r', '', text) # remove one or more Windows-line-break\n",
    "    text = re.sub(r'\\u3000', ' ', text) # convert white space: \\u3000    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_1(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\t+', '', text) # remove one or more Tab\n",
    "    text = re.sub(r'\\f+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r'\\v+', '', text) # remove one or more special Space\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    \n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_some_whitespace_2(text): # Does not remove normal Space\n",
    "#     sentence = re.sub(r'\\W+', '#', sentence)\n",
    "    text = re.sub(r'\\n+', ' ', text) # Change one or more \\n to a Space, this is to merge sentences within paragraph\n",
    "    text = re.sub(r' +', ' ', text) # merge two or more Spaces to 1 Space\n",
    "    text = re.sub(r'(\\^\\*\\#)( +)(\\#\\*\\^)', '^*##*^', text) # remove one or more Spaces between Paragraph-Tags or Sentence-Tags\n",
    "    \n",
    "    text = re.sub(r'(\\#\\*\\^S\\^\\*\\#)+', '#*^S^*#', text) # merge two or more sentence-Tags -> 1 Sentence-Tag\n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "    \n",
    "    # remove a Sentence-Tag immediately before a Paragraph-Tag\n",
    "    text = re.sub(r'(\\#\\*\\^S\\^\\*\\#)( *)(\\#\\*\\^P\\^\\*\\#)', '#*^P^*#', text) \n",
    "\n",
    "    # remove lead & tail spaces:\n",
    "    text =text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define Paragraph-Tag =  \n",
    "#   #*^P^*#\n",
    "\n",
    "### Define Sentence-Tag =  \n",
    "#   #*^S^*#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each paragraph\n",
    "def tag_paragraph(text):\n",
    "    text = re.sub(r'((\\n ) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Spaces\n",
    "    text = re.sub(r'((\\n\\t) +)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + two or more Tabs\n",
    "    text = re.sub(r'(\\n( *)\\n)+', '#*^P^*#', text) # Tag paragraph, pattern: \\n + zero or more Spaces + \\n\n",
    "    text = re.sub(r'(\\#\\*\\^P\\^\\*\\#)+', '#*^P^*#', text) # merge two or more Paragraph-Tags -> 1 Paragraph-Tag\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add a special tag to end of each sentence\n",
    "def tag_sentence(text):\n",
    "    text = re.sub(r'ã€‚+', 'ã€‚#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'ï¼+', 'ï¼#*^S^*#', text) # Tag sentence - Chinese\n",
    "    text = re.sub(r'\\ï¼Ÿ+', 'ï¼Ÿ#*^S^*#', text) # Tag sentence - Chinese\n",
    "#     text = re.sub(r'ï¼›+', 'ï¼›#*^S^*#', text) # Tag sentence - Chinese\n",
    "\n",
    "    # 2017 MAR 24\n",
    "    text = re.sub(r'(\\.)( +)', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'(!)( +)', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?( +)', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r'(;)( +)', ';#*^S^*#', text) # Tag sentence - English\n",
    "\n",
    "    text = re.sub(r'\\.\\n', '.#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'!\\n', '!#*^S^*#', text) # Tag sentence - English\n",
    "    text = re.sub(r'\\?\\n', '?#*^S^*#', text) # Tag sentence - English\n",
    "#     text = re.sub(r';\\n', ';#*^S^*#', text) # Tag sentence - English\n",
    "    \n",
    "    # remove a Sentence-Tag immediately before an ending quotation\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#â€™', 'â€™#*^S^*#', text) # Chinese â€™\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#â€', 'â€#*^S^*#', text) # Chinese â€\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\\'', '\\'#*^S^*#', text) # English '\n",
    "    text = re.sub(r'\\#\\*\\^S\\^\\*\\#\"', '\"#*^S^*#', text) # English \"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = linebreak_conversion_win_linux(content)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = tag_paragraph(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_1(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = tag_sentence(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "content_format = clean_some_whitespace_2(content_format)\n",
    "# content_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(content_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transfer tagged text to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a text into paragraphs\n",
    "def split_article_to_paragraphs(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^P^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split a paragraph into sentences\n",
    "def split_paragraph_to_sentences(text):\n",
    "#     text = text.replace(\"#*^P^*#\", \"#*^S^*#\") # convert Paragraph-Tag        \n",
    "    return text.split(\"#*^S^*#\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1st loop Paragraphs list, 2nd loop Sentences list\n",
    "# create a few new columns, then write into dataframe, together with original Sentence string\n",
    "\n",
    "# define empty dataframe:\n",
    "df_article = pd.DataFrame(columns=('sentence', \n",
    "                                   'word_count', # sentence word count, including punctuations \n",
    "                                   'sentence_id', # unique sentence s/n within an article\n",
    "                                   'sentence_id_paragraph',  # sentence s/n within a paragraph \n",
    "                                   'paragraph_id', \n",
    "                                   'class_rank', \n",
    "                                   'score_word', # score based on word tf-idf\n",
    "                                   'score_sentence', # score based on intersection of sentence pairs\n",
    "                                   'score_word_norm', # Normalized score\n",
    "                                   'score_sentence_norm', # Normalized score\n",
    "                                   'score',\n",
    "                                  ))\n",
    "df_sentence_id = 0\n",
    "\n",
    "# split_article_to_paragraphs:\n",
    "article_paragraphs = split_article_to_paragraphs(content_format)\n",
    "\n",
    "for i in range(0, len(article_paragraphs)):\n",
    "    # split_paragraph_to_sentences:\n",
    "    article_paragraphs_sentences = split_paragraph_to_sentences(article_paragraphs[i].strip())\n",
    "\n",
    "    for j in range(0, len(article_paragraphs_sentences)):\n",
    "        if article_paragraphs_sentences[j].strip() != '':\n",
    "            df_sentence_id = df_sentence_id + 1\n",
    "            # write to dataframe:\n",
    "            df_article.loc[len(df_article)] = [article_paragraphs_sentences[j].strip(), \n",
    "                                               len(article_paragraphs_sentences[j].strip()), \n",
    "                                               df_sentence_id, \n",
    "                                               j+1, \n",
    "                                               i+1, \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '', \n",
    "                                               '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assume the 1st sentence as Title of Article\n",
    "\n",
    "title = df_article['sentence'][0]\n",
    "print('Title of Article : %s' % title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_article['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KudosData_word_tokenizer\n",
    "df_article['sentence_tokenized'] = df_article['sentence'].apply(lambda x: KudosData_word_tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure no empty sentences:\n",
    "print('Number of empty sentences in dataframe: %d ' % len(df_article[df_article['sentence_tokenized'] == '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove invalid empty sentences\n",
    "print(len(df_article))\n",
    "df_article=df_article[df_article['sentence_tokenized'] != '']\n",
    "df_article = df_article.sort_values(by=['sentence_id'],).reset_index(drop=True)\n",
    "print(len(df_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KudosData_word_count\n",
    "df_article['sentence_tf'] = df_article['sentence'].apply(lambda x: KudosData_word_count(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = df_article['sentence_tokenized']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# my_stopword_list = ['and','to','the','of', 'in']\n",
    "#vectorizer = TfidfVectorizer(stop_words=my_stopword_list)\n",
    "\n",
    "# choice of no nomalization of tfidf output (not recommended)\n",
    "#vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# TF-IDF score\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# IDF score\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "\n",
    "# TF is in df_article[['sentence_tf']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### æŠŠTF-iDFæ•°å€¼èµ‹äºˆç›¸å¯¹åº”çš„è¯ç»„\n",
    "tfidf = tfidf.tocsr()\n",
    "\n",
    "n_docs = tfidf.shape[0]\n",
    "tfidftables = [{} for _ in range(n_docs)]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, j in zip(*tfidf.nonzero()):\n",
    "    tfidftables[i][terms[j]] = tfidf[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Document-Term-Matrix's TF-IDF matrix size:\n",
    "print ('This tfidf matrix is a very large table: [ %d rows/docs X %d columns/words ]' \n",
    "       % (tfidf.shape[0], tfidf.shape[1]))\n",
    "print ('It contains %d eliments: one score per word per document !'\n",
    "       % (tfidf.shape[0] * tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add tfidf score into dataframe \n",
    "df_article['tfidf'] = tfidftables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[['sentence', 'sentence_tokenized', 'sentence_tf', 'tfidf']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate importance score for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (1)\n",
    "### Calculate score_word for each sentence, based on sentence word_count tf-idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiment: use tf-idf and len(sentence_tokenized) to calculate score\n",
    "# tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "\n",
    "for i in range(0,len(df_article)):\n",
    "    if len(df_article['tfidf'][i]) == 0:\n",
    "        df_article['score_word'][i] = 0\n",
    "    else:\n",
    "        tmp_sum = 0\n",
    "        for key, values in df_article['tfidf'][i].items():\n",
    "            tmp_sum += values\n",
    "        \n",
    "        tmp_mean = tmp_sum / len(df_article['sentence_tokenized'][i])\n",
    "        df_article['score_word'][i] = tmp_mean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring (2)\n",
    "### Calculate score_sentence for each sentence, based on pair-wise sentence comparison/intersection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Caculate raw intersection score between pair of two sentences, from df_article['sentence_tokenized']\n",
    "def sentences_intersection(sent1tokenized, sent2tokenized):\n",
    "    # www.KudosData.com - Chinese\n",
    "    # split the sentence into words/tokens\n",
    "    s1 = set(sent1tokenized.split(\" \"))\n",
    "    s2 = set(sent2tokenized.split(\" \"))\n",
    "\n",
    "    # If there is not intersection, just return 0\n",
    "    if (len(s1) + len(s2)) == 0:\n",
    "        print('# If there is not intersection, just return 0')\n",
    "        return 0\n",
    "\n",
    "    # Normalize the result by the average number of words\n",
    "    return len(s1.intersection(s2)) / ((len(s1) + len(s2)) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate important score of every pair of sentences\n",
    "\n",
    "n = len(df_article['sentence_tokenized'])\n",
    "        \n",
    "# [Sam python 2.7 -> 3.4] values = [[0 for x in xrange(n)] for x in xrange(n)]\n",
    "df_score_raw_values = [[0 for x in range(n)] for x in range(n)]\n",
    "for i in range(0, n):\n",
    "    for j in range(0, n):\n",
    "        df_score_raw_values[i][j] = sentences_intersection(df_article['sentence_tokenized'][i], \n",
    "                                                           df_article['sentence_tokenized'][j])\n",
    "\n",
    "# The score of a sentence is the sum of all its intersection\n",
    "sentences_dic = {}\n",
    "\n",
    "for i in range(0, n):\n",
    "    df_score = 0\n",
    "    for j in range(0, n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        df_score += df_score_raw_values[i][j]\n",
    "    df_article['score_sentence'][i] = df_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data (Internal use,  not for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å›¾è¡¨æ˜¾ç¤ºï¼š\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'å›¾')  \n",
    "plt.xlabel(u'Xåæ ‡ï¼šSentence word_count')  \n",
    "plt.ylabel(u'Yåæ ‡ï¼šSentence frequency')  \n",
    "# df_article['word_count'].value_counts().sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "df_article['word_count'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å›¾è¡¨æ˜¾ç¤ºï¼š\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'å›¾')  \n",
    "plt.xlabel(u'Xåæ ‡ï¼šParagraph_id')  \n",
    "plt.ylabel(u'Yåæ ‡ï¼šSentence frequency')  \n",
    "df_article['paragraph_id'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å›¾è¡¨æ˜¾ç¤ºï¼š\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'å›¾')  \n",
    "plt.xlabel(u'Xåæ ‡ï¼šsentence_id_paragraph')  \n",
    "plt.ylabel(u'Yåæ ‡ï¼šSentence frequency')  \n",
    "df_article['sentence_id_paragraph'].hist(bins=100)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# å›¾è¡¨æ˜¾ç¤ºï¼š\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'å›¾')  \n",
    "plt.xlabel(u'Xåæ ‡ï¼šscore_word')  \n",
    "plt.ylabel(u'Yåæ ‡ï¼šfrequency')  \n",
    "df_article['score_word'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# å›¾è¡¨æ˜¾ç¤ºï¼š\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(u'å›¾')  \n",
    "plt.xlabel(u'Xåæ ‡ï¼šscore_sentence')  \n",
    "plt.ylabel(u'Yåæ ‡ï¼šfrequency')  \n",
    "df_article['score_sentence'].hist(bins = 100)\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "#plt.xlim(0,0.5)\n",
    "#plt.ylim(0,0.5)\n",
    "# plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score_word'] > 0.15) & (df_article['score_word'] < 0.25)]\n",
    "# df_article[(df_article['score_word'] > 0.2)].sort_values(by=['score_sentence', 'score_word'], ascending=[False, False,])\n",
    "# df_article[(df_article['score_sentence'] > 250)].sort_values(by=['score_word', 'score_sentence'], ascending=[False, False,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# log(score_word)\n",
    "df_article['score_word_log'] = np.log(df_article['score_word'].astype('float64') + \n",
    "                                      df_article[df_article['score_word'] >0 ]['score_word'].min()/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_word_log - Zero mean, unit variance\n",
    "\n",
    "# df_article['score_word_norm'] = (df_article['score_word'] - df_article['score_word'].mean()) / df_article['score_word'].std()\n",
    "df_article['score_word_norm'] = (df_article['score_word_log'] - df_article['score_word_log'].mean()) / df_article['score_word_log'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_word_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize score_sentence - Zero mean, unit variance\n",
    "\n",
    "df_article['score_sentence_norm'] = (df_article['score_sentence'] - df_article['score_sentence'].mean()) / df_article['score_sentence'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score_sentence_norm'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate class_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score integration\n",
    "# df_article['score'] = (df_article['score_sentence_norm'] + df_article['score_word_norm']) / 2\n",
    "\n",
    "# Sam Gu: 23 Mar 2017 - Experiment found that the score_word, which is based on tf-idf, doesn't seem to work well.\n",
    "#                       score_word     tends to favor short sentences\n",
    "#                       score_sentence tends to favor long  sentences\n",
    "#                       Hence, here we use score_sentence only for final scoring.\n",
    "\n",
    "# df_article['score'] = (df_article['score_word'] + df_article['score_sentence'] ) / 2\n",
    "df_article['score'] = df_article['score_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Min-Max normalization:\n",
    "df_article['score'] = (df_article['score'] - df_article['score'].min()) / (df_article['score'].max() -df_article['score'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_article['score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort firstly\n",
    "df_article = df_article.sort_values(by=['paragraph_id', 'score'], ascending=[True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below step runs long time... Tuning needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate Class_Rank\n",
    "\n",
    "current_class_rank = 0\n",
    "current_paragraph_id = 0\n",
    "\n",
    "for i in range(0, len(df_article)):\n",
    "    if df_article['paragraph_id'][i] != current_paragraph_id: # change of Paragraph, thus reset class_rank\n",
    "        current_class_rank = 1\n",
    "        current_paragraph_id = df_article['paragraph_id'][i]\n",
    "    else:\n",
    "        current_class_rank = current_class_rank + 1\n",
    "        \n",
    "    df_article['class_rank'][i] = current_class_rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article = df_article.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_article[['sentence',\n",
    "           'paragraph_id',\n",
    "           'sentence_id_paragraph',\n",
    "           'class_rank',\n",
    "           'score',\n",
    "           'sentence_tokenized'\n",
    "          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_article[(df_article['score'] == 0) | (df_article['score'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract results based on user parameters:\n",
    "* Max number of words\n",
    "* % of original number of words\n",
    "* Max lines of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataframe copy\n",
    "# Currently, the two dataframes are exactly the same.\n",
    "df_article_internal = pd.DataFrame.copy(df_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "# total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "# print('total_words_original_article : ', total_words_original_article)\n",
    "# print('total_words_internal_article : ', total_words_internal_article)\n",
    "# print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sam Gu: experiment shows no major improvement to use code in this block:\n",
    "\n",
    "'''\n",
    "\n",
    "# Heuristic cleaning:\n",
    "# 1.Remove sentences, which has only one valid word. \n",
    "# 2.Remove paragraph, which has only single sentence.\n",
    "\n",
    "# 1.\n",
    "df_article_internal = df_article_internal[df_article_internal['sentence_tokenized'].map(len) > 1]\n",
    "print('*** www.KudosData.com *** Removed number of sentences, which has only one valid word : %d'\n",
    "      % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# 2.\n",
    "df_article_internal_paragraph = df_article_internal['paragraph_id'].value_counts().to_frame(name = 'sentence_count')\n",
    "df_article_internal_paragraph = df_article_internal_paragraph[df_article_internal_paragraph['sentence_count'] > 1]\n",
    "valid_paragraph_id = df_article_internal_paragraph.index.tolist()\n",
    "df_article_internal = df_article_internal[df_article_internal['paragraph_id'].isin(valid_paragraph_id)] \n",
    "print('*** www.KudosData.com *** Removed number of sentences in total : %d' % (len(df_article) - len(df_article_internal)))\n",
    "\n",
    "# sort Dataframe to 'result lookup mode'\n",
    "df_article_internal = df_article_internal.sort_values(by=['class_rank', 'score', 'paragraph_id', 'sentence_id'], \n",
    "                                    ascending=[True, False, True, True]).reset_index(drop=True)\n",
    "# Above sort a must sort !!! for below processing:\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Accept user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# valid range: >= 0\n",
    "parm_max_word = 200\n",
    "\n",
    "# valid range: >= 0\n",
    "parm_max_sentence = 10\n",
    "\n",
    "# valid range: [0, 100%]\n",
    "parm_max_percent = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Validation of user parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (isinstance(parm_max_word, int) | isinstance(parm_max_word, float)):\n",
    "    if parm_max_word >= 0:\n",
    "        print('!1! valid input parm_max_word : ', parm_max_word)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_word : ', parm_max_word)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_word : ', parm_max_word)\n",
    "\n",
    "if (isinstance(parm_max_sentence, int) | isinstance(parm_max_sentence, float)):\n",
    "    if parm_max_sentence >= 0:\n",
    "        print('!1! valid input parm_max_sentence : ', parm_max_sentence)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_sentence : ', parm_max_sentence)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_sentence : ', parm_max_sentence)\n",
    "\n",
    "if (isinstance(parm_max_percent, int) | isinstance(parm_max_percent, float)):\n",
    "    if parm_max_percent >= 0:\n",
    "        print('!1! valid input parm_max_percent : ', parm_max_percent)\n",
    "    else:\n",
    "        print('!2! Invalid input parm_max_percent : ', parm_max_percent)    \n",
    "else:\n",
    "    print('!3! Invalid input parm_max_percent : ', parm_max_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_percent\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word / total_words_original_article <= parm_max_percent:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "sum_current_percent = sum_current_word / total_words_original_article\n",
    "print('---------- cut by parm_max_percent :')\n",
    "print('sum_current_word  / total_words_original_article:', sum_current_percent)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_word\n",
    "\n",
    "# Loop Dataframe, accumulate length of sentences, stop when parm_max_word reached, return the index(), cut dataframe to display\n",
    "\n",
    "sum_current_word = 0\n",
    "cut_index = len(df_article_internal['sentence'])\n",
    "\n",
    "# print('Start loop...')\n",
    "for s in range(0, len(df_article_internal['sentence'])):\n",
    "#     print('s : %d' % s)\n",
    "    if sum_current_word <= parm_max_word:\n",
    "        sum_current_word += len(df_article_internal['sentence'][s])\n",
    "    else:\n",
    "#         stop, return index number\n",
    "        cut_index = s - 1\n",
    "        sum_current_word -= len(df_article_internal['sentence'][s-1])\n",
    "#         print('To break')\n",
    "        break\n",
    "\n",
    "# print('End loop')\n",
    "print('---------- cut by parm_max_word :')\n",
    "print('sum_current_word :', sum_current_word)\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cut by parm_max_sentence\n",
    "\n",
    "cut_index = parm_max_sentence\n",
    "\n",
    "print('---------- cut by parm_max_sentence :')\n",
    "print('cut_index : ', cut_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract top number of sentences as summary, based on: cut_index\n",
    "df_article_final = df_article_internal[0:cut_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sort by original sentence order \n",
    "df_article_final = df_article_final.sort_values(by=['sentence_id'], ascending=[True])\n",
    "df_article_final[['sentence_id', 'sentence', 'score', 'class_rank', 'paragraph_id', 'sentence_id_paragraph']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total_words_original_article = df_article['sentence'].map(len).sum()\n",
    "# total_words_internal_article = df_article_internal['sentence'].map(len).sum()\n",
    "total_words_article_summary  = df_article_final['sentence'].map(len).sum()\n",
    "\n",
    "print('total_words_original_article : ', total_words_original_article)\n",
    "print('total_words_internal_article : ', total_words_internal_article)\n",
    "print('total_words_article_summary  : ', total_words_article_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print('\\n'.join(list(df_article_final['sentence'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with io.open('output_topic_summary.txt','w',encoding='utf8') as f:\n",
    "#     f.write(\"Original Length : %s\" % total_words_original_article)\n",
    "    f.write(\"No. Paragraphs  : %d\" % df_article_internal['paragraph_id'].max())\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Original Length : %s\" % total_words_internal_article)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Summary  Length : %s\" % total_words_article_summary)\n",
    "    f.write(\"\\n\")\n",
    "#     f.write(\"Summary  Ratio  : %s %%\" % (100 * (sum_current_word / total_words_original_article)))\n",
    "    f.write(\"Summary  Ratio  : %.2f %%\" % (100 * (total_words_article_summary / total_words_internal_article)))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Title of Article: %s\" % title)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    f.write('\\n'.join(list(df_article_final['sentence'])))\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is to check if there is sentence with very few valid/real word, should have very low score.\n",
    "df_article[['sentence', 'word_count', 'sentence_tokenized', 'tfidf', 'score']][df_article['sentence_tokenized'].map(len) <= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
